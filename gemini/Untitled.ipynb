{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4f40b0-1aac-4f00-b40e-b35d2bc501b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1757630774.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install google-generativeai\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d85a9d4-b963-490c-aa17-6ad55eea7a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f33d8f-6194-4dfb-ac80-980eb899b5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (25.1.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.1.1\n",
      "    Uninstalling pip-25.1.1:\n",
      "      Successfully uninstalled pip-25.1.1\n",
      "Successfully installed pip-25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.13.exe and pip3.exe are installed in 'C:\\Users\\18esh\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f093ff-badd-4f89-a6d3-03993d62e777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a light switch. It can be either ON or OFF, right?  That's how regular computers work – bits are either 0 or 1.\n",
      "\n",
      "Quantum computers use **qubits**.  A qubit is like a special light switch that can be ON, OFF, or *both at the same time*! This \"both at the same time\" is called **superposition**.  It's like the light is both on and off until you actually look at it, then it \"chooses\" one state.\n",
      "\n",
      "Another key idea is **entanglement**. Imagine two of these special light switches linked magically. If one is ON, the other is instantly OFF, no matter how far apart they are.  Entangled qubits are linked in a similar way.  Changing one instantly affects the other.\n",
      "\n",
      "Because qubits can be in multiple states at once and are interconnected, quantum computers can explore many possibilities simultaneously. This allows them to solve certain types of problems much faster than even the most powerful regular computers.  Think of it like searching a maze: a regular computer tries each path one by one, while a quantum computer can explore all paths at once.\n",
      "\n",
      "However, quantum computers aren't meant to replace regular computers entirely. They're specialized tools for tackling specific, very complex problems in areas like:\n",
      "\n",
      "* **Drug discovery:** Simulating molecules to design new drugs.\n",
      "* **Materials science:** Designing new materials with specific properties.\n",
      "* **Financial modeling:** Developing more accurate financial models.\n",
      "* **Cryptography:** Breaking current encryption methods (and creating new, unbreakable ones).\n",
      "\n",
      "They're still in their early stages of development, but the potential is enormous.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAo9E2DL9tEQcDAyQbWwf-5QVCriyU7jIQ\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Explain quantum computing simply.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d8a0dc-2667-47aa-ae44-64fb5121343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "print(model.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef147be-5b27-4d19-9676-597d3986ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Super Agent Ready! Type a math question or 'exit' to quit.\n",
      "\n",
      "📊 Agent Flow Diagram:\n",
      "\n",
      "[classify]\n",
      "   |\n",
      "   v\n",
      "[route]\n",
      "\n",
      "[route]\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Your question:  1+2=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 [Super Agent] Classifying tool for input: 1+2=\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "404 models/gemini-2.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFound\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 235\u001b[39m\n\u001b[32m    232\u001b[39m         log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[43msuper_agent_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36msuper_agent_loop\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    207\u001b[39m state = {\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser_input\u001b[39m\u001b[33m\"\u001b[39m: user_text,\n\u001b[32m    209\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtool_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m: history.copy()\n\u001b[32m    212\u001b[39m }\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):  \u001b[38;5;66;03m# up to 3 chaining attempts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     history.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33muser_input\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    218\u001b[39m     history.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2844\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2841\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2842\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2844\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2845\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2848\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2849\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2853\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2857\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2534\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2532\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2533\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2534\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2541\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2544\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langgraph\\pregel\\runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langgraph\\pregel\\retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mclassify_tool\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    107\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[33mYou are a tool selector agent.\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[33mDecide which tool best matches the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms input.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33mTool:\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    125\u001b[39m     model = genai.GenerativeModel(\u001b[33m\"\u001b[39m\u001b[33mmodels/gemini-2.5-flash-latest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     tool_name = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m.text.strip()\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m TOOL_AGENTS:\n\u001b[32m    129\u001b[39m         log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚠️ [Super Agent] Invalid tool selected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, defaulting to fallback_tool\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mNotFound\u001b[39m: 404 models/gemini-2.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
      "During task with name 'classify' and id '0df4a837-f5c7-a20b-e0cf-4502d1d3595c'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, Dict, Callable, Optional\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=\"AIzaSyAo9E2DL9tEQcDAyQbWwf-5QVCriyU7jIQ\")\n",
    "\n",
    "LOG_FILE = \"super_agent.log\"\n",
    "\n",
    "def log(message: str):\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:  # <-- encoding specified\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "\n",
    "# --- Modular Tools ---\n",
    "\n",
    "def basic_math_tool(expr: str) -> str:\n",
    "    log(f\"\\n🔧 [basic_math_tool] Received input: {expr}\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Convert the following natural language into a valid Python math expression using only numbers and operators.\n",
    "Only output the expression — no explanation.\n",
    "\n",
    "Input: \"{expr}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "    model = genai.GenerativeModel(\"models/gemini-2.5-flash-latest\")\n",
    "    gen_response = model.generate_content(prompt)\n",
    "    math_expr = gen_response.text.strip()\n",
    "\n",
    "    log(f\"🧮 [basic_math_tool] Parsed math expression: {math_expr}\")\n",
    "\n",
    "    # Simple safety check\n",
    "    if not re.fullmatch(r\"[\\d\\s\\+\\-\\*\\/\\%\\(\\)\\.]+\", math_expr):\n",
    "        log(\"❌ [basic_math_tool] Unsafe expression detected.\")\n",
    "        return \"Math Error: Unsafe expression.\"\n",
    "\n",
    "    try:\n",
    "        result = str(eval(math_expr))\n",
    "        log(f\"✅ [basic_math_tool] Evaluated result: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        log(f\"❌ [basic_math_tool] Evaluation failed: {e}\")\n",
    "        return f\"Math Error: {e}\"\n",
    "\n",
    "def modulus_tool(expr: str) -> str:\n",
    "    log(f\"\\n🔧 [modulus_tool] Received input: {expr}\")\n",
    "    try:\n",
    "        a, b = map(int, expr.split('%'))\n",
    "        result = str(a % b)\n",
    "        log(f\"✅ [modulus_tool] Result: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        log(f\"❌ [modulus_tool] Error: {e}\")\n",
    "        return f\"Modulus Error: {e}\"\n",
    "\n",
    "def percentage_tool(expr: str) -> str:\n",
    "    log(f\"\\n🔧 [percentage_tool] Received input: {expr}\")\n",
    "    try:\n",
    "        numbers = list(map(float, re.findall(r\"[\\d.]+\", expr)))\n",
    "        if len(numbers) < 2:\n",
    "            raise ValueError(\"Need two numeric values\")\n",
    "        value, total = numbers[0], numbers[1]\n",
    "        result = str((value / 100) * total)\n",
    "        log(f\"✅ [percentage_tool] Result: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        log(f\"❌ [percentage_tool] Error: {e}\")\n",
    "        return f\"Percentage Error: {e}\"\n",
    "\n",
    "def fallback_tool(expr: str) -> str:\n",
    "    log(f\"\\n🔧 [fallback_tool] Using LLM fallback for: {expr}\")\n",
    "    prompt = f\"Answer this math question or help with: {expr}\"\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "response = model.generate_content(prompt).text.strip()\n",
    "    log(f\"✅ [fallback_tool] Result: {response}\")\n",
    "    return response\n",
    "\n",
    "TOOL_AGENTS: Dict[str, Callable[[str], str]] = {\n",
    "    \"basic_math_tool\": basic_math_tool,\n",
    "    \"modulus_tool\": modulus_tool,\n",
    "    \"percentage_tool\": percentage_tool,\n",
    "    \"fallback_tool\": fallback_tool\n",
    "}\n",
    "\n",
    "# --- Agent State ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    tool_name: str\n",
    "    result: str\n",
    "    history: Optional[list[str]]\n",
    "\n",
    "# --- Classify tool ---\n",
    "\n",
    "def classify_tool(state: AgentState) -> AgentState:\n",
    "    user_input = state['user_input']\n",
    "    history = state.get('history') or []\n",
    "    log(f\"\\n🔍 [Super Agent] Classifying tool for input: {user_input}\")\n",
    "\n",
    "    history_text = \"\\n\".join(history[-5:])  # last 5 lines for context\n",
    "    prompt = f\"\"\"\n",
    "You are a tool selector agent.\n",
    "Decide which tool best matches the user's input.\n",
    "\n",
    "Available tools:\n",
    "- basic_math_tool: for math like \"12 + 7 * 2\", or \"add 4 to 5\"\n",
    "- modulus_tool: for expressions using %, like \"10 % 3\"\n",
    "- percentage_tool: for inputs like \"25% of 320\" or \"25 percentage 320\"\n",
    "\n",
    "Only return one tool name exactly as shown.\n",
    "\n",
    "Conversation history:\n",
    "{history_text}\n",
    "\n",
    "Input: \"{user_input}\"\n",
    "Tool:\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"models/gemini-2.5-flash-latest\")\n",
    "    tool_name = model.generate_content(prompt).text.strip()\n",
    "\n",
    "    if tool_name not in TOOL_AGENTS:\n",
    "        log(f\"⚠️ [Super Agent] Invalid tool selected: {tool_name}, defaulting to fallback_tool\")\n",
    "        tool_name = \"fallback_tool\"\n",
    "    else:\n",
    "        log(f\"🧠 [Super Agent] Selected tool: {tool_name}\")\n",
    "\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"tool_name\": tool_name,\n",
    "        \"result\": \"\",\n",
    "        \"history\": history\n",
    "    }\n",
    "\n",
    "# --- Run selected tool ---\n",
    "\n",
    "def run_selected_tool(state: AgentState) -> AgentState:\n",
    "    tool_name = state[\"tool_name\"]\n",
    "    user_input = state[\"user_input\"]\n",
    "    log(f\"\\n🔄 [Router] Routing to: {tool_name}\")\n",
    "\n",
    "    tool_fn = TOOL_AGENTS.get(tool_name)\n",
    "    if not tool_fn:\n",
    "        log(f\"⚠️ [Router] Unknown tool '{tool_name}', using fallback.\")\n",
    "        result = fallback_tool(user_input)\n",
    "    else:\n",
    "        result = tool_fn(user_input)\n",
    "\n",
    "    return {**state, \"result\": result}\n",
    "\n",
    "# --- Build Graph ---\n",
    "\n",
    "def build_super_agent():\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"classify\", classify_tool)\n",
    "    graph.add_node(\"route\", run_selected_tool)\n",
    "\n",
    "    graph.set_entry_point(\"classify\")\n",
    "    graph.add_edge(\"classify\", \"route\")\n",
    "    graph.set_finish_point(\"route\")\n",
    "\n",
    "    # Print flow diagram slowly here\n",
    "    print_flow_diagram([\"classify\", \"route\"], [(\"classify\", \"route\")])\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "# --- Print flow diagram with arrows and delay ---\n",
    "\n",
    "def print_flow_diagram(nodes, edges, delay=0.7):\n",
    "    print(\"\\n📊 Agent Flow Diagram:\\n\")\n",
    "\n",
    "    for node in nodes:\n",
    "        print(f\"[{node}]\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        next_nodes = [dst for src, dst in edges if src == node]\n",
    "        for nxt in next_nodes:\n",
    "            print(\"   |\")\n",
    "            time.sleep(delay / 2)\n",
    "            print(\"   v\")\n",
    "            time.sleep(delay / 2)\n",
    "            print(f\"[{nxt}]\")\n",
    "            time.sleep(delay)\n",
    "        print()\n",
    "\n",
    "# --- Main loop ---\n",
    "\n",
    "def super_agent_loop():\n",
    "    history = []\n",
    "\n",
    "    log(\"\\n🤖 Super Agent Ready! Type a math question or 'exit' to quit.\")\n",
    "\n",
    "    graph = build_super_agent()\n",
    "\n",
    "    while True:\n",
    "        user_text = input(\"\\n💬 Your question: \").strip()\n",
    "        if user_text.lower() == \"exit\":\n",
    "            log(\"👋 Exiting Super Agent. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        state = {\n",
    "            \"user_input\": user_text,\n",
    "            \"tool_name\": \"\",\n",
    "            \"result\": \"\",\n",
    "            \"history\": history.copy()\n",
    "        }\n",
    "\n",
    "        for _ in range(3):  # up to 3 chaining attempts\n",
    "            state = graph.invoke(state)\n",
    "\n",
    "            history.append(f\"Q: {state['user_input']}\")\n",
    "            history.append(f\"A: {state['result']}\")\n",
    "\n",
    "            # Re-classify if result looks like expression\n",
    "            if state[\"tool_name\"] == \"basic_math_tool\":\n",
    "                if re.fullmatch(r\"[\\d\\s\\+\\-\\*\\/\\%\\(\\)\\.]+\", state[\"result\"]) and state[\"result\"] != state[\"user_input\"]:\n",
    "                    log(\"🔄 [Super Agent] Detected expression in result, re-classifying.\")\n",
    "                    state[\"user_input\"] = state[\"result\"]\n",
    "                    state[\"tool_name\"] = \"\"\n",
    "                    state[\"result\"] = \"\"\n",
    "                    continue\n",
    "            break\n",
    "\n",
    "        log(\"\\n🧠 [Final Output]\")\n",
    "        log(f\"🔧 Selected Tool: {state['tool_name']}\")\n",
    "        log(f\"✅ Result: {state['result']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    super_agent_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8632dd-f602-4c16-8cd2-fa5d6c280b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: google-generativeai 0.8.5\n",
      "Uninstalling google-generativeai-0.8.5:\n",
      "  Successfully uninstalled google-generativeai-0.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4400a99c-e434-4907-9c09-dd7e2e644f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: google-generativeai\n"
     ]
    }
   ],
   "source": [
    "!pip show google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75816974-dd73-4565-8ca1-102cca4f5495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.176.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (4.14.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core->google-generativeai) (1.69.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core->google-generativeai) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python313\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Installing collected packages: google-generativeai\n",
      "Successfully installed google-generativeai-0.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d832112-adc0-41fa-bb27-73d08d9f1eb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m genai.configure(api_key=\u001b[33m\"\u001b[39m\u001b[33myour_existing_key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m models = genai.list_models()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\generativeai\\models.py:204\u001b[39m, in \u001b[36mlist_models\u001b[39m\u001b[34m(page_size, client, request_options)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    202\u001b[39m     client = get_default_model_client()\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    205\u001b[39m     model = \u001b[38;5;28mtype\u001b[39m(model).to_dict(model)\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_types.Model(**model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\client.py:923\u001b[39m, in \u001b[36mModelServiceClient.list_models\u001b[39m\u001b[34m(self, request, page_size, page_token, retry, timeout, metadata)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# This method is paged; wrap the response in a pager, which provides\u001b[39;00m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# an `__iter__` convenience method.\u001b[39;00m\n\u001b[32m    932\u001b[39m response = pagers.ListModelsPager(\n\u001b[32m    933\u001b[39m     method=rpc,\n\u001b[32m    934\u001b[39m     request=request,\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m     metadata=metadata,\n\u001b[32m    939\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"your_existing_key\")\n",
    "\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b311a1e-5866-46aa-8403-266a14530119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Use your Gemini API key here\n",
    "genai.configure(api_key=\"AIzaSyAo9E2DL9tEQcDAyQbWwf-5QVCriyU7jIQ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba66e0c0-1c3e-4112-9127-33a5a206149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-2.5-flash-lite\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-preview\n",
      "models/veo-3.0-fast-generate-preview\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n"
     ]
    }
   ],
   "source": [
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf4272d-2c41-44ff-a08f-201cfa3f6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (1.68.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdfbc243-97ef-4dbf-a48a-8e4a5601e72b",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 🔑 Replace with your actual OpenAI API key\u001b[39;00m\n\u001b[32m      4\u001b[39m openai.api_key = \u001b[33m\"\u001b[39m\u001b[33msk-proj-XHdSBz36wqTfvXuTXgEt3u5G7wJ7u7wRvnp7Ld3ZtSYcT2fVyohEaLIwDDoI5aTQcwxI-Zr9nDT3BlbkFJ1Dw2gcoC6Wv27QsAQSW-811o5ap14AEqWW-q_tw1wsGrdQ7dxc0qKJXlBiHvUMvMHD3S_V8dMA\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4-1106-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPT-4.1 model\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain quantum computing in simple terms.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# 🔑 Replace with your actual OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-1106-preview\",  # GPT-4.1 model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a25922-2284-4462-a15d-c959321a1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 1.68.2\n",
      "Uninstalling openai-1.68.2:\n",
      "  Successfully uninstalled openai-1.68.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96995334-daf2-4122-87b3-7741ab6434cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-1.98.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\18esh\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.98.0-py3-none-any.whl (767 kB)\n",
      "   ---------------------------------------- 0.0/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------------------- ------------ 524.3/767.7 kB 214.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 524.3/767.7 kB 214.9 kB/s eta 0:00:02\n",
      "   ------------------------- ------------ 524.3/767.7 kB 214.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 767.7/767.7 kB 271.5 kB/s  0:00:01\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.98.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\18esh\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "834d6891-b92e-4ec3-916e-75ffc7c975e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4-1106-preview` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m client = OpenAI(api_key=\u001b[33m\"\u001b[39m\u001b[33msk-proj-XHdSBz36wqTfvXuTXgEt3u5G7wJ7u7wRvnp7Ld3ZtSYcT2fVyohEaLIwDDoI5aTQcwxI-Zr9nDT3BlbkFJ1Dw2gcoC6Wv27QsAQSW-811o5ap14AEqWW-q_tw1wsGrdQ7dxc0qKJXlBiHvUMvMHD3S_V8dMA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4-1106-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain quantum computing simply.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[39m, in \u001b[36mcreate\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    803\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    843\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    844\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03m    **Starting a new project?** We recommend trying\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03m    [Responses](https://platform.openai.com/docs/api-reference/responses) to take\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[33;03m    advantage of the latest OpenAI platform features. Compare\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[33;03m    [Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).\u001b[39;00m\n\u001b[32m    850\u001b[39m \n\u001b[32m    851\u001b[39m \u001b[33;03m    ---\u001b[39;00m\n\u001b[32m    852\u001b[39m \n\u001b[32m    853\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation. Learn more in the\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m    [text generation](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[33;03m    [vision](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[33;03m    [audio](https://platform.openai.com/docs/guides/audio) guides.\u001b[39;00m\n\u001b[32m    857\u001b[39m \n\u001b[32m    858\u001b[39m \u001b[33;03m    Parameter support can differ depending on the model used to generate the\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[33;03m    response, particularly for newer reasoning models. Parameters that are only\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[33;03m    supported for reasoning models are noted below. For the current state of\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03m    unsupported parameters in reasoning models,\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03m    [refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m      messages: A list of messages comprising the conversation so far. Depending on the\u001b[39;00m\n\u001b[32m    866\u001b[39m \u001b[33;03m          [model](https://platform.openai.com/docs/models) you use, different message\u001b[39;00m\n\u001b[32m    867\u001b[39m \u001b[33;03m          types (modalities) are supported, like\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[33;03m          [text](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[32m    869\u001b[39m \u001b[33;03m          [images](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[32m    870\u001b[39m \u001b[33;03m          [audio](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[32m    871\u001b[39m \n\u001b[32m    872\u001b[39m \u001b[33;03m      model: Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\u001b[39;00m\n\u001b[32m    873\u001b[39m \u001b[33;03m          wide range of models with different capabilities, performance characteristics,\u001b[39;00m\n\u001b[32m    874\u001b[39m \u001b[33;03m          and price points. Refer to the\u001b[39;00m\n\u001b[32m    875\u001b[39m \u001b[33;03m          [model guide](https://platform.openai.com/docs/models) to browse and compare\u001b[39;00m\n\u001b[32m    876\u001b[39m \u001b[33;03m          available models.\u001b[39;00m\n\u001b[32m    877\u001b[39m \n\u001b[32m    878\u001b[39m \u001b[33;03m      stream: If set to true, the model response data will be streamed to the client as it is\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m          generated using\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[33;03m          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\u001b[39;00m\n\u001b[32m    881\u001b[39m \u001b[33;03m          See the\u001b[39;00m\n\u001b[32m    882\u001b[39m \u001b[33;03m          [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)\u001b[39;00m\n\u001b[32m    883\u001b[39m \u001b[33;03m          for more information, along with the\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m          [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[33;03m          guide for more information on how to handle the streaming events.\u001b[39;00m\n\u001b[32m    886\u001b[39m \n\u001b[32m    887\u001b[39m \u001b[33;03m      audio: Parameters for audio output. Required when audio output is requested with\u001b[39;00m\n\u001b[32m    888\u001b[39m \u001b[33;03m          `modalities: [\"audio\"]`.\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[32m    890\u001b[39m \n\u001b[32m    891\u001b[39m \u001b[33;03m      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\u001b[39;00m\n\u001b[32m    892\u001b[39m \u001b[33;03m          existing frequency in the text so far, decreasing the model's likelihood to\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[33;03m          repeat the same line verbatim.\u001b[39;00m\n\u001b[32m    894\u001b[39m \n\u001b[32m    895\u001b[39m \u001b[33;03m      function_call: Deprecated in favor of `tool_choice`.\u001b[39;00m\n\u001b[32m    896\u001b[39m \n\u001b[32m    897\u001b[39m \u001b[33;03m          Controls which (if any) function is called by the model.\u001b[39;00m\n\u001b[32m    898\u001b[39m \n\u001b[32m    899\u001b[39m \u001b[33;03m          `none` means the model will not call a function and instead generates a message.\u001b[39;00m\n\u001b[32m    900\u001b[39m \n\u001b[32m    901\u001b[39m \u001b[33;03m          `auto` means the model can pick between generating a message or calling a\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m          function.\u001b[39;00m\n\u001b[32m    903\u001b[39m \n\u001b[32m    904\u001b[39m \u001b[33;03m          Specifying a particular function via `{\"name\": \"my_function\"}` forces the model\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[33;03m          to call that function.\u001b[39;00m\n\u001b[32m    906\u001b[39m \n\u001b[32m    907\u001b[39m \u001b[33;03m          `none` is the default when no functions are present. `auto` is the default if\u001b[39;00m\n\u001b[32m    908\u001b[39m \u001b[33;03m          functions are present.\u001b[39;00m\n\u001b[32m    909\u001b[39m \n\u001b[32m    910\u001b[39m \u001b[33;03m      functions: Deprecated in favor of `tools`.\u001b[39;00m\n\u001b[32m    911\u001b[39m \n\u001b[32m    912\u001b[39m \u001b[33;03m          A list of functions the model may generate JSON inputs for.\u001b[39;00m\n\u001b[32m    913\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m \u001b[33;03m      logit_bias: Modify the likelihood of specified tokens appearing in the completion.\u001b[39;00m\n\u001b[32m    915\u001b[39m \n\u001b[32m    916\u001b[39m \u001b[33;03m          Accepts a JSON object that maps tokens (specified by their token ID in the\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[33;03m          tokenizer) to an associated bias value from -100 to 100. Mathematically, the\u001b[39;00m\n\u001b[32m    918\u001b[39m \u001b[33;03m          bias is added to the logits generated by the model prior to sampling. The exact\u001b[39;00m\n\u001b[32m    919\u001b[39m \u001b[33;03m          effect will vary per model, but values between -1 and 1 should decrease or\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[33;03m          increase likelihood of selection; values like -100 or 100 should result in a ban\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[33;03m          or exclusive selection of the relevant token.\u001b[39;00m\n\u001b[32m    922\u001b[39m \n\u001b[32m    923\u001b[39m \u001b[33;03m      logprobs: Whether to return log probabilities of the output tokens or not. If true,\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[33;03m          returns the log probabilities of each output token returned in the `content` of\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[33;03m          `message`.\u001b[39;00m\n\u001b[32m    926\u001b[39m \n\u001b[32m    927\u001b[39m \u001b[33;03m      max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion,\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m          including visible output tokens and\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m          [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[32m    930\u001b[39m \n\u001b[32m    931\u001b[39m \u001b[33;03m      max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the chat\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[33;03m          completion. This value can be used to control\u001b[39;00m\n\u001b[32m    933\u001b[39m \u001b[33;03m          [costs](https://openai.com/api/pricing/) for text generated via API.\u001b[39;00m\n\u001b[32m    934\u001b[39m \n\u001b[32m    935\u001b[39m \u001b[33;03m          This value is now deprecated in favor of `max_completion_tokens`, and is not\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[33;03m          compatible with\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[33;03m          [o-series models](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[32m    938\u001b[39m \n\u001b[32m    939\u001b[39m \u001b[33;03m      metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03m          for storing additional information about the object in a structured format, and\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m          querying for objects via API or the dashboard.\u001b[39;00m\n\u001b[32m    942\u001b[39m \n\u001b[32m    943\u001b[39m \u001b[33;03m          Keys are strings with a maximum length of 64 characters. Values are strings with\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m          a maximum length of 512 characters.\u001b[39;00m\n\u001b[32m    945\u001b[39m \n\u001b[32m    946\u001b[39m \u001b[33;03m      modalities: Output types that you would like the model to generate. Most models are capable\u001b[39;00m\n\u001b[32m    947\u001b[39m \u001b[33;03m          of generating text, which is the default:\u001b[39;00m\n\u001b[32m    948\u001b[39m \n\u001b[32m    949\u001b[39m \u001b[33;03m          `[\"text\"]`\u001b[39;00m\n\u001b[32m    950\u001b[39m \n\u001b[32m    951\u001b[39m \u001b[33;03m          The `gpt-4o-audio-preview` model can also be used to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[33;03m          [generate audio](https://platform.openai.com/docs/guides/audio). To request that\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[33;03m          this model generate both text and audio responses, you can use:\u001b[39;00m\n\u001b[32m    954\u001b[39m \n\u001b[32m    955\u001b[39m \u001b[33;03m          `[\"text\", \"audio\"]`\u001b[39;00m\n\u001b[32m    956\u001b[39m \n\u001b[32m    957\u001b[39m \u001b[33;03m      n: How many chat completion choices to generate for each input message. Note that\u001b[39;00m\n\u001b[32m    958\u001b[39m \u001b[33;03m          you will be charged based on the number of generated tokens across all of the\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m          choices. Keep `n` as `1` to minimize costs.\u001b[39;00m\n\u001b[32m    960\u001b[39m \n\u001b[32m    961\u001b[39m \u001b[33;03m      parallel_tool_calls: Whether to enable\u001b[39;00m\n\u001b[32m    962\u001b[39m \u001b[33;03m          [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[33;03m          during tool use.\u001b[39;00m\n\u001b[32m    964\u001b[39m \n\u001b[32m    965\u001b[39m \u001b[33;03m      prediction: Static predicted output content, such as the content of a text file that is\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[33;03m          being regenerated.\u001b[39;00m\n\u001b[32m    967\u001b[39m \n\u001b[32m    968\u001b[39m \u001b[33;03m      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\u001b[39;00m\n\u001b[32m    969\u001b[39m \u001b[33;03m          whether they appear in the text so far, increasing the model's likelihood to\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m          talk about new topics.\u001b[39;00m\n\u001b[32m    971\u001b[39m \n\u001b[32m    972\u001b[39m \u001b[33;03m      prompt_cache_key: Used by OpenAI to cache responses for similar requests to optimize your cache\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[33;03m          hit rates. Replaces the `user` field.\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\u001b[39;00m\n\u001b[32m    975\u001b[39m \n\u001b[32m    976\u001b[39m \u001b[33;03m      reasoning_effort: **o-series models only**\u001b[39;00m\n\u001b[32m    977\u001b[39m \n\u001b[32m    978\u001b[39m \u001b[33;03m          Constrains effort on reasoning for\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[33;03m          [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[33;03m          supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[33;03m          result in faster responses and fewer tokens used on reasoning in a response.\u001b[39;00m\n\u001b[32m    982\u001b[39m \n\u001b[32m    983\u001b[39m \u001b[33;03m      response_format: An object specifying the format that the model must output.\u001b[39;00m\n\u001b[32m    984\u001b[39m \n\u001b[32m    985\u001b[39m \u001b[33;03m          Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03m          Outputs which ensures the model will match your supplied JSON schema. Learn more\u001b[39;00m\n\u001b[32m    987\u001b[39m \u001b[33;03m          in the\u001b[39;00m\n\u001b[32m    988\u001b[39m \u001b[33;03m          [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\u001b[39;00m\n\u001b[32m    989\u001b[39m \n\u001b[32m    990\u001b[39m \u001b[33;03m          Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[33;03m          ensures the message the model generates is valid JSON. Using `json_schema` is\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[33;03m          preferred for models that support it.\u001b[39;00m\n\u001b[32m    993\u001b[39m \n\u001b[32m    994\u001b[39m \u001b[33;03m      safety_identifier: A stable identifier used to help detect users of your application that may be\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[33;03m          violating OpenAI's usage policies. The IDs should be a string that uniquely\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[33;03m          identifies each user. We recommend hashing their username or email address, in\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[33;03m          order to avoid sending us any identifying information.\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\u001b[39;00m\n\u001b[32m    999\u001b[39m \n\u001b[32m   1000\u001b[39m \u001b[33;03m      seed: This feature is in Beta. If specified, our system will make a best effort to\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[33;03m          sample deterministically, such that repeated requests with the same `seed` and\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[33;03m          parameters should return the same result. Determinism is not guaranteed, and you\u001b[39;00m\n\u001b[32m   1003\u001b[39m \u001b[33;03m          should refer to the `system_fingerprint` response parameter to monitor changes\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[33;03m          in the backend.\u001b[39;00m\n\u001b[32m   1005\u001b[39m \n\u001b[32m   1006\u001b[39m \u001b[33;03m      service_tier: Specifies the processing type used for serving the request.\u001b[39;00m\n\u001b[32m   1007\u001b[39m \n\u001b[32m   1008\u001b[39m \u001b[33;03m          - If set to 'auto', then the request will be processed with the service tier\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[33;03m            configured in the Project settings. Unless otherwise configured, the Project\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[33;03m            will use 'default'.\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[33;03m          - If set to 'default', then the request will be processed with the standard\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[33;03m            pricing and performance for the selected model.\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[33;03m          - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[33;03m            'priority', then the request will be processed with the corresponding service\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[33;03m            tier. [Contact sales](https://openai.com/contact-sales) to learn more about\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[33;03m            Priority processing.\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m          - When not set, the default behavior is 'auto'.\u001b[39;00m\n\u001b[32m   1018\u001b[39m \n\u001b[32m   1019\u001b[39m \u001b[33;03m          When the `service_tier` parameter is set, the response body will include the\u001b[39;00m\n\u001b[32m   1020\u001b[39m \u001b[33;03m          `service_tier` value based on the processing mode actually used to serve the\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[33;03m          request. This response value may be different from the value set in the\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m          parameter.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \n\u001b[32m   1024\u001b[39m \u001b[33;03m      stop: Not supported with latest reasoning models `o3` and `o4-mini`.\u001b[39;00m\n\u001b[32m   1025\u001b[39m \n\u001b[32m   1026\u001b[39m \u001b[33;03m          Up to 4 sequences where the API will stop generating further tokens. The\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[33;03m          returned text will not contain the stop sequence.\u001b[39;00m\n\u001b[32m   1028\u001b[39m \n\u001b[32m   1029\u001b[39m \u001b[33;03m      store: Whether or not to store the output of this chat completion request for use in\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[33;03m          our [model distillation](https://platform.openai.com/docs/guides/distillation)\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[33;03m          or [evals](https://platform.openai.com/docs/guides/evals) products.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \n\u001b[32m   1033\u001b[39m \u001b[33;03m          Supports text and image inputs. Note: image inputs over 10MB will be dropped.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \n\u001b[32m   1035\u001b[39m \u001b[33;03m      stream_options: Options for streaming response. Only set this when you set `stream: true`.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \n\u001b[32m   1037\u001b[39m \u001b[33;03m      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[33;03m          make the output more random, while lower values like 0.2 will make it more\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[33;03m          focused and deterministic. We generally recommend altering this or `top_p` but\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m          not both.\u001b[39;00m\n\u001b[32m   1041\u001b[39m \n\u001b[32m   1042\u001b[39m \u001b[33;03m      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[33;03m          not call any tool and instead generates a message. `auto` means the model can\u001b[39;00m\n\u001b[32m   1044\u001b[39m \u001b[33;03m          pick between generating a message or calling one or more tools. `required` means\u001b[39;00m\n\u001b[32m   1045\u001b[39m \u001b[33;03m          the model must call one or more tools. Specifying a particular tool via\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[33;03m          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\u001b[39;00m\n\u001b[32m   1047\u001b[39m \u001b[33;03m          call that tool.\u001b[39;00m\n\u001b[32m   1048\u001b[39m \n\u001b[32m   1049\u001b[39m \u001b[33;03m          `none` is the default when no tools are present. `auto` is the default if tools\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m          are present.\u001b[39;00m\n\u001b[32m   1051\u001b[39m \n\u001b[32m   1052\u001b[39m \u001b[33;03m      tools: A list of tools the model may call. Currently, only functions are supported as a\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[33;03m          tool. Use this to provide a list of functions the model may generate JSON inputs\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[33;03m          for. A max of 128 functions are supported.\u001b[39;00m\n\u001b[32m   1055\u001b[39m \n\u001b[32m   1056\u001b[39m \u001b[33;03m      top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m          return at each token position, each with an associated log probability.\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m          `logprobs` must be set to `true` if this parameter is used.\u001b[39;00m\n\u001b[32m   1059\u001b[39m \n\u001b[32m   1060\u001b[39m \u001b[33;03m      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m          model considers the results of the tokens with top_p probability mass. So 0.1\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[33;03m          means only the tokens comprising the top 10% probability mass are considered.\u001b[39;00m\n\u001b[32m   1063\u001b[39m \n\u001b[32m   1064\u001b[39m \u001b[33;03m          We generally recommend altering this or `temperature` but not both.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \n\u001b[32m   1066\u001b[39m \u001b[33;03m      user: This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use\u001b[39;00m\n\u001b[32m   1067\u001b[39m \u001b[33;03m          `prompt_cache_key` instead to maintain caching optimizations. A stable\u001b[39;00m\n\u001b[32m   1068\u001b[39m \u001b[33;03m          identifier for your end-users. Used to boost cache hit rates by better bucketing\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[33;03m          similar requests and to help OpenAI detect and prevent abuse.\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\u001b[39;00m\n\u001b[32m   1071\u001b[39m \n\u001b[32m   1072\u001b[39m \u001b[33;03m      web_search_options: This tool searches the web for relevant results to use in a response. Learn more\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m          about the\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m          [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\u001b[39;00m\n\u001b[32m   1075\u001b[39m \n\u001b[32m   1076\u001b[39m \u001b[33;03m      extra_headers: Send extra headers\u001b[39;00m\n\u001b[32m   1077\u001b[39m \n\u001b[32m   1078\u001b[39m \u001b[33;03m      extra_query: Add additional query parameters to the request\u001b[39;00m\n\u001b[32m   1079\u001b[39m \n\u001b[32m   1080\u001b[39m \u001b[33;03m      extra_body: Add additional JSON properties to the request\u001b[39;00m\n\u001b[32m   1081\u001b[39m \n\u001b[32m   1082\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1084\u001b[39m     ...\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1229\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1231\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m   1239\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1240\u001b[39m ) -> ResponseT | _StreamT: ...\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1245\u001b[39m     *,\n\u001b[32m   1246\u001b[39m     cast_to: Type[ResponseT],\n\u001b[32m   1247\u001b[39m     body: Body | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1248\u001b[39m     options: RequestOptions = {},\n\u001b[32m   1249\u001b[39m     files: RequestFiles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1250\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m   1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:919\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    911\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    915\u001b[39m     stream: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m],\n\u001b[32m    916\u001b[39m     stream_cls: Type[_StreamT],\n\u001b[32m    917\u001b[39m ) -> _StreamT: ...\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    921\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    922\u001b[39m     cast_to: Type[ResponseT],\n\u001b[32m    923\u001b[39m     options: FinalRequestOptions,\n\u001b[32m    924\u001b[39m     *,\n\u001b[32m    925\u001b[39m     stream: Literal[\u001b[38;5;28;01mFalse\u001b[39;00m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    926\u001b[39m ) -> ResponseT: ...\n\u001b[32m    928\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    935\u001b[39m     stream_cls: Type[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1023\u001b[39m, in \u001b[36m_request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1013\u001b[39m log.debug(\n\u001b[32m   1014\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1015\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m     response.headers,\n\u001b[32m   1020\u001b[39m )\n\u001b[32m   1021\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1024\u001b[39m     response.raise_for_status()\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `gpt-4-1106-preview` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing simply.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ae15816-be0c-4be6-89f4-6c0caaaecb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002\n",
      "whisper-1\n",
      "gpt-3.5-turbo\n",
      "tts-1\n",
      "gpt-3.5-turbo-16k\n",
      "davinci-002\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-3.5-turbo-1106\n",
      "tts-1-hd\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "o1-mini-2024-09-12\n",
      "o1-mini\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "omni-moderation-latest\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "gpt-4o-mini-tts\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-image-1\n",
      "gpt-4o-audio-preview-2025-06-03\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print all model IDs\n",
    "for model in models.data:\n",
    "    print(model.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8d989ef-79d5-44a6-8431-1395c10e1b0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m client = OpenAI(api_key=\u001b[33m\"\u001b[39m\u001b[33msk-proj-XHdSBz36wqTfvXuTXgEt3u5G7wJ7u7wRvnp7Ld3ZtSYcT2fVyohEaLIwDDoI5aTQcwxI-Zr9nDT3BlbkFJ1Dw2gcoC6Wv27QsAQSW-811o5ap14AEqWW-q_tw1wsGrdQ7dxc0qKJXlBiHvUMvMHD3S_V8dMA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 🎯 You have access to this\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain quantum computing simply.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[39m, in \u001b[36mcreate\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    803\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    843\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    844\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03m    **Starting a new project?** We recommend trying\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03m    [Responses](https://platform.openai.com/docs/api-reference/responses) to take\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[33;03m    advantage of the latest OpenAI platform features. Compare\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[33;03m    [Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).\u001b[39;00m\n\u001b[32m    850\u001b[39m \n\u001b[32m    851\u001b[39m \u001b[33;03m    ---\u001b[39;00m\n\u001b[32m    852\u001b[39m \n\u001b[32m    853\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation. Learn more in the\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m    [text generation](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[33;03m    [vision](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[33;03m    [audio](https://platform.openai.com/docs/guides/audio) guides.\u001b[39;00m\n\u001b[32m    857\u001b[39m \n\u001b[32m    858\u001b[39m \u001b[33;03m    Parameter support can differ depending on the model used to generate the\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[33;03m    response, particularly for newer reasoning models. Parameters that are only\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[33;03m    supported for reasoning models are noted below. For the current state of\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03m    unsupported parameters in reasoning models,\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03m    [refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m      messages: A list of messages comprising the conversation so far. Depending on the\u001b[39;00m\n\u001b[32m    866\u001b[39m \u001b[33;03m          [model](https://platform.openai.com/docs/models) you use, different message\u001b[39;00m\n\u001b[32m    867\u001b[39m \u001b[33;03m          types (modalities) are supported, like\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[33;03m          [text](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[32m    869\u001b[39m \u001b[33;03m          [images](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[32m    870\u001b[39m \u001b[33;03m          [audio](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[32m    871\u001b[39m \n\u001b[32m    872\u001b[39m \u001b[33;03m      model: Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a\u001b[39;00m\n\u001b[32m    873\u001b[39m \u001b[33;03m          wide range of models with different capabilities, performance characteristics,\u001b[39;00m\n\u001b[32m    874\u001b[39m \u001b[33;03m          and price points. Refer to the\u001b[39;00m\n\u001b[32m    875\u001b[39m \u001b[33;03m          [model guide](https://platform.openai.com/docs/models) to browse and compare\u001b[39;00m\n\u001b[32m    876\u001b[39m \u001b[33;03m          available models.\u001b[39;00m\n\u001b[32m    877\u001b[39m \n\u001b[32m    878\u001b[39m \u001b[33;03m      stream: If set to true, the model response data will be streamed to the client as it is\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m          generated using\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[33;03m          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\u001b[39;00m\n\u001b[32m    881\u001b[39m \u001b[33;03m          See the\u001b[39;00m\n\u001b[32m    882\u001b[39m \u001b[33;03m          [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)\u001b[39;00m\n\u001b[32m    883\u001b[39m \u001b[33;03m          for more information, along with the\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m          [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[33;03m          guide for more information on how to handle the streaming events.\u001b[39;00m\n\u001b[32m    886\u001b[39m \n\u001b[32m    887\u001b[39m \u001b[33;03m      audio: Parameters for audio output. Required when audio output is requested with\u001b[39;00m\n\u001b[32m    888\u001b[39m \u001b[33;03m          `modalities: [\"audio\"]`.\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[32m    890\u001b[39m \n\u001b[32m    891\u001b[39m \u001b[33;03m      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\u001b[39;00m\n\u001b[32m    892\u001b[39m \u001b[33;03m          existing frequency in the text so far, decreasing the model's likelihood to\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[33;03m          repeat the same line verbatim.\u001b[39;00m\n\u001b[32m    894\u001b[39m \n\u001b[32m    895\u001b[39m \u001b[33;03m      function_call: Deprecated in favor of `tool_choice`.\u001b[39;00m\n\u001b[32m    896\u001b[39m \n\u001b[32m    897\u001b[39m \u001b[33;03m          Controls which (if any) function is called by the model.\u001b[39;00m\n\u001b[32m    898\u001b[39m \n\u001b[32m    899\u001b[39m \u001b[33;03m          `none` means the model will not call a function and instead generates a message.\u001b[39;00m\n\u001b[32m    900\u001b[39m \n\u001b[32m    901\u001b[39m \u001b[33;03m          `auto` means the model can pick between generating a message or calling a\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m          function.\u001b[39;00m\n\u001b[32m    903\u001b[39m \n\u001b[32m    904\u001b[39m \u001b[33;03m          Specifying a particular function via `{\"name\": \"my_function\"}` forces the model\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[33;03m          to call that function.\u001b[39;00m\n\u001b[32m    906\u001b[39m \n\u001b[32m    907\u001b[39m \u001b[33;03m          `none` is the default when no functions are present. `auto` is the default if\u001b[39;00m\n\u001b[32m    908\u001b[39m \u001b[33;03m          functions are present.\u001b[39;00m\n\u001b[32m    909\u001b[39m \n\u001b[32m    910\u001b[39m \u001b[33;03m      functions: Deprecated in favor of `tools`.\u001b[39;00m\n\u001b[32m    911\u001b[39m \n\u001b[32m    912\u001b[39m \u001b[33;03m          A list of functions the model may generate JSON inputs for.\u001b[39;00m\n\u001b[32m    913\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m \u001b[33;03m      logit_bias: Modify the likelihood of specified tokens appearing in the completion.\u001b[39;00m\n\u001b[32m    915\u001b[39m \n\u001b[32m    916\u001b[39m \u001b[33;03m          Accepts a JSON object that maps tokens (specified by their token ID in the\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[33;03m          tokenizer) to an associated bias value from -100 to 100. Mathematically, the\u001b[39;00m\n\u001b[32m    918\u001b[39m \u001b[33;03m          bias is added to the logits generated by the model prior to sampling. The exact\u001b[39;00m\n\u001b[32m    919\u001b[39m \u001b[33;03m          effect will vary per model, but values between -1 and 1 should decrease or\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[33;03m          increase likelihood of selection; values like -100 or 100 should result in a ban\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[33;03m          or exclusive selection of the relevant token.\u001b[39;00m\n\u001b[32m    922\u001b[39m \n\u001b[32m    923\u001b[39m \u001b[33;03m      logprobs: Whether to return log probabilities of the output tokens or not. If true,\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[33;03m          returns the log probabilities of each output token returned in the `content` of\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[33;03m          `message`.\u001b[39;00m\n\u001b[32m    926\u001b[39m \n\u001b[32m    927\u001b[39m \u001b[33;03m      max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion,\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m          including visible output tokens and\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m          [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[32m    930\u001b[39m \n\u001b[32m    931\u001b[39m \u001b[33;03m      max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the chat\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[33;03m          completion. This value can be used to control\u001b[39;00m\n\u001b[32m    933\u001b[39m \u001b[33;03m          [costs](https://openai.com/api/pricing/) for text generated via API.\u001b[39;00m\n\u001b[32m    934\u001b[39m \n\u001b[32m    935\u001b[39m \u001b[33;03m          This value is now deprecated in favor of `max_completion_tokens`, and is not\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[33;03m          compatible with\u001b[39;00m\n\u001b[32m    937\u001b[39m \u001b[33;03m          [o-series models](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[32m    938\u001b[39m \n\u001b[32m    939\u001b[39m \u001b[33;03m      metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03m          for storing additional information about the object in a structured format, and\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m          querying for objects via API or the dashboard.\u001b[39;00m\n\u001b[32m    942\u001b[39m \n\u001b[32m    943\u001b[39m \u001b[33;03m          Keys are strings with a maximum length of 64 characters. Values are strings with\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m          a maximum length of 512 characters.\u001b[39;00m\n\u001b[32m    945\u001b[39m \n\u001b[32m    946\u001b[39m \u001b[33;03m      modalities: Output types that you would like the model to generate. Most models are capable\u001b[39;00m\n\u001b[32m    947\u001b[39m \u001b[33;03m          of generating text, which is the default:\u001b[39;00m\n\u001b[32m    948\u001b[39m \n\u001b[32m    949\u001b[39m \u001b[33;03m          `[\"text\"]`\u001b[39;00m\n\u001b[32m    950\u001b[39m \n\u001b[32m    951\u001b[39m \u001b[33;03m          The `gpt-4o-audio-preview` model can also be used to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[33;03m          [generate audio](https://platform.openai.com/docs/guides/audio). To request that\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[33;03m          this model generate both text and audio responses, you can use:\u001b[39;00m\n\u001b[32m    954\u001b[39m \n\u001b[32m    955\u001b[39m \u001b[33;03m          `[\"text\", \"audio\"]`\u001b[39;00m\n\u001b[32m    956\u001b[39m \n\u001b[32m    957\u001b[39m \u001b[33;03m      n: How many chat completion choices to generate for each input message. Note that\u001b[39;00m\n\u001b[32m    958\u001b[39m \u001b[33;03m          you will be charged based on the number of generated tokens across all of the\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m          choices. Keep `n` as `1` to minimize costs.\u001b[39;00m\n\u001b[32m    960\u001b[39m \n\u001b[32m    961\u001b[39m \u001b[33;03m      parallel_tool_calls: Whether to enable\u001b[39;00m\n\u001b[32m    962\u001b[39m \u001b[33;03m          [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[33;03m          during tool use.\u001b[39;00m\n\u001b[32m    964\u001b[39m \n\u001b[32m    965\u001b[39m \u001b[33;03m      prediction: Static predicted output content, such as the content of a text file that is\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[33;03m          being regenerated.\u001b[39;00m\n\u001b[32m    967\u001b[39m \n\u001b[32m    968\u001b[39m \u001b[33;03m      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\u001b[39;00m\n\u001b[32m    969\u001b[39m \u001b[33;03m          whether they appear in the text so far, increasing the model's likelihood to\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m          talk about new topics.\u001b[39;00m\n\u001b[32m    971\u001b[39m \n\u001b[32m    972\u001b[39m \u001b[33;03m      prompt_cache_key: Used by OpenAI to cache responses for similar requests to optimize your cache\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[33;03m          hit rates. Replaces the `user` field.\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\u001b[39;00m\n\u001b[32m    975\u001b[39m \n\u001b[32m    976\u001b[39m \u001b[33;03m      reasoning_effort: **o-series models only**\u001b[39;00m\n\u001b[32m    977\u001b[39m \n\u001b[32m    978\u001b[39m \u001b[33;03m          Constrains effort on reasoning for\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[33;03m          [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[33;03m          supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[33;03m          result in faster responses and fewer tokens used on reasoning in a response.\u001b[39;00m\n\u001b[32m    982\u001b[39m \n\u001b[32m    983\u001b[39m \u001b[33;03m      response_format: An object specifying the format that the model must output.\u001b[39;00m\n\u001b[32m    984\u001b[39m \n\u001b[32m    985\u001b[39m \u001b[33;03m          Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03m          Outputs which ensures the model will match your supplied JSON schema. Learn more\u001b[39;00m\n\u001b[32m    987\u001b[39m \u001b[33;03m          in the\u001b[39;00m\n\u001b[32m    988\u001b[39m \u001b[33;03m          [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\u001b[39;00m\n\u001b[32m    989\u001b[39m \n\u001b[32m    990\u001b[39m \u001b[33;03m          Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[33;03m          ensures the message the model generates is valid JSON. Using `json_schema` is\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[33;03m          preferred for models that support it.\u001b[39;00m\n\u001b[32m    993\u001b[39m \n\u001b[32m    994\u001b[39m \u001b[33;03m      safety_identifier: A stable identifier used to help detect users of your application that may be\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[33;03m          violating OpenAI's usage policies. The IDs should be a string that uniquely\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[33;03m          identifies each user. We recommend hashing their username or email address, in\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[33;03m          order to avoid sending us any identifying information.\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\u001b[39;00m\n\u001b[32m    999\u001b[39m \n\u001b[32m   1000\u001b[39m \u001b[33;03m      seed: This feature is in Beta. If specified, our system will make a best effort to\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[33;03m          sample deterministically, such that repeated requests with the same `seed` and\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[33;03m          parameters should return the same result. Determinism is not guaranteed, and you\u001b[39;00m\n\u001b[32m   1003\u001b[39m \u001b[33;03m          should refer to the `system_fingerprint` response parameter to monitor changes\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[33;03m          in the backend.\u001b[39;00m\n\u001b[32m   1005\u001b[39m \n\u001b[32m   1006\u001b[39m \u001b[33;03m      service_tier: Specifies the processing type used for serving the request.\u001b[39;00m\n\u001b[32m   1007\u001b[39m \n\u001b[32m   1008\u001b[39m \u001b[33;03m          - If set to 'auto', then the request will be processed with the service tier\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[33;03m            configured in the Project settings. Unless otherwise configured, the Project\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[33;03m            will use 'default'.\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[33;03m          - If set to 'default', then the request will be processed with the standard\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[33;03m            pricing and performance for the selected model.\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[33;03m          - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[33;03m            'priority', then the request will be processed with the corresponding service\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[33;03m            tier. [Contact sales](https://openai.com/contact-sales) to learn more about\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[33;03m            Priority processing.\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m          - When not set, the default behavior is 'auto'.\u001b[39;00m\n\u001b[32m   1018\u001b[39m \n\u001b[32m   1019\u001b[39m \u001b[33;03m          When the `service_tier` parameter is set, the response body will include the\u001b[39;00m\n\u001b[32m   1020\u001b[39m \u001b[33;03m          `service_tier` value based on the processing mode actually used to serve the\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[33;03m          request. This response value may be different from the value set in the\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m          parameter.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \n\u001b[32m   1024\u001b[39m \u001b[33;03m      stop: Not supported with latest reasoning models `o3` and `o4-mini`.\u001b[39;00m\n\u001b[32m   1025\u001b[39m \n\u001b[32m   1026\u001b[39m \u001b[33;03m          Up to 4 sequences where the API will stop generating further tokens. The\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[33;03m          returned text will not contain the stop sequence.\u001b[39;00m\n\u001b[32m   1028\u001b[39m \n\u001b[32m   1029\u001b[39m \u001b[33;03m      store: Whether or not to store the output of this chat completion request for use in\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[33;03m          our [model distillation](https://platform.openai.com/docs/guides/distillation)\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[33;03m          or [evals](https://platform.openai.com/docs/guides/evals) products.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \n\u001b[32m   1033\u001b[39m \u001b[33;03m          Supports text and image inputs. Note: image inputs over 10MB will be dropped.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \n\u001b[32m   1035\u001b[39m \u001b[33;03m      stream_options: Options for streaming response. Only set this when you set `stream: true`.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \n\u001b[32m   1037\u001b[39m \u001b[33;03m      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[33;03m          make the output more random, while lower values like 0.2 will make it more\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[33;03m          focused and deterministic. We generally recommend altering this or `top_p` but\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m          not both.\u001b[39;00m\n\u001b[32m   1041\u001b[39m \n\u001b[32m   1042\u001b[39m \u001b[33;03m      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[33;03m          not call any tool and instead generates a message. `auto` means the model can\u001b[39;00m\n\u001b[32m   1044\u001b[39m \u001b[33;03m          pick between generating a message or calling one or more tools. `required` means\u001b[39;00m\n\u001b[32m   1045\u001b[39m \u001b[33;03m          the model must call one or more tools. Specifying a particular tool via\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[33;03m          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\u001b[39;00m\n\u001b[32m   1047\u001b[39m \u001b[33;03m          call that tool.\u001b[39;00m\n\u001b[32m   1048\u001b[39m \n\u001b[32m   1049\u001b[39m \u001b[33;03m          `none` is the default when no tools are present. `auto` is the default if tools\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m          are present.\u001b[39;00m\n\u001b[32m   1051\u001b[39m \n\u001b[32m   1052\u001b[39m \u001b[33;03m      tools: A list of tools the model may call. Currently, only functions are supported as a\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[33;03m          tool. Use this to provide a list of functions the model may generate JSON inputs\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[33;03m          for. A max of 128 functions are supported.\u001b[39;00m\n\u001b[32m   1055\u001b[39m \n\u001b[32m   1056\u001b[39m \u001b[33;03m      top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m          return at each token position, each with an associated log probability.\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m          `logprobs` must be set to `true` if this parameter is used.\u001b[39;00m\n\u001b[32m   1059\u001b[39m \n\u001b[32m   1060\u001b[39m \u001b[33;03m      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m          model considers the results of the tokens with top_p probability mass. So 0.1\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[33;03m          means only the tokens comprising the top 10% probability mass are considered.\u001b[39;00m\n\u001b[32m   1063\u001b[39m \n\u001b[32m   1064\u001b[39m \u001b[33;03m          We generally recommend altering this or `temperature` but not both.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \n\u001b[32m   1066\u001b[39m \u001b[33;03m      user: This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use\u001b[39;00m\n\u001b[32m   1067\u001b[39m \u001b[33;03m          `prompt_cache_key` instead to maintain caching optimizations. A stable\u001b[39;00m\n\u001b[32m   1068\u001b[39m \u001b[33;03m          identifier for your end-users. Used to boost cache hit rates by better bucketing\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[33;03m          similar requests and to help OpenAI detect and prevent abuse.\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03m          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\u001b[39;00m\n\u001b[32m   1071\u001b[39m \n\u001b[32m   1072\u001b[39m \u001b[33;03m      web_search_options: This tool searches the web for relevant results to use in a response. Learn more\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m          about the\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m          [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\u001b[39;00m\n\u001b[32m   1075\u001b[39m \n\u001b[32m   1076\u001b[39m \u001b[33;03m      extra_headers: Send extra headers\u001b[39;00m\n\u001b[32m   1077\u001b[39m \n\u001b[32m   1078\u001b[39m \u001b[33;03m      extra_query: Add additional query parameters to the request\u001b[39;00m\n\u001b[32m   1079\u001b[39m \n\u001b[32m   1080\u001b[39m \u001b[33;03m      extra_body: Add additional JSON properties to the request\u001b[39;00m\n\u001b[32m   1081\u001b[39m \n\u001b[32m   1082\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1084\u001b[39m     ...\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1229\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1231\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m   1239\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1240\u001b[39m ) -> ResponseT | _StreamT: ...\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1245\u001b[39m     *,\n\u001b[32m   1246\u001b[39m     cast_to: Type[ResponseT],\n\u001b[32m   1247\u001b[39m     body: Body | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1248\u001b[39m     options: RequestOptions = {},\n\u001b[32m   1249\u001b[39m     files: RequestFiles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1250\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m   1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:919\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    911\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    915\u001b[39m     stream: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m],\n\u001b[32m    916\u001b[39m     stream_cls: Type[_StreamT],\n\u001b[32m    917\u001b[39m ) -> _StreamT: ...\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    921\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    922\u001b[39m     cast_to: Type[ResponseT],\n\u001b[32m    923\u001b[39m     options: FinalRequestOptions,\n\u001b[32m    924\u001b[39m     *,\n\u001b[32m    925\u001b[39m     stream: Literal[\u001b[38;5;28;01mFalse\u001b[39;00m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    926\u001b[39m ) -> ResponseT: ...\n\u001b[32m    928\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    935\u001b[39m     stream_cls: Type[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1008\u001b[39m, in \u001b[36m_request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28mself\u001b[39m._sleep_for_retry(\n\u001b[32m   1003\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1004\u001b[39m         max_retries=max_retries,\n\u001b[32m   1005\u001b[39m         options=input_options,\n\u001b[32m   1006\u001b[39m         response=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1007\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1010\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36m_retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1008\u001b[39m, in \u001b[36m_request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28mself\u001b[39m._sleep_for_retry(\n\u001b[32m   1003\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1004\u001b[39m         max_retries=max_retries,\n\u001b[32m   1005\u001b[39m         options=input_options,\n\u001b[32m   1006\u001b[39m         response=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1007\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1010\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36m_retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1023\u001b[39m, in \u001b[36m_request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1013\u001b[39m log.debug(\n\u001b[32m   1014\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1015\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m     response.headers,\n\u001b[32m   1020\u001b[39m )\n\u001b[32m   1021\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1024\u001b[39m     response.raise_for_status()\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # 🎯 You have access to this\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing simply.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ab16acc-3780-4526-8f04-d3ca3ab8a449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary\n",
      "1. Perplexity has deprecated or removed the \"pplx-\" series models (including pplx-7b-online and pplx-70b-online) as of March 2025, instructing users to switch to the newer \"Sonar\" models such as sonar-7b-online, sonar-7b, or sonar-70b.\n",
      "2. Attempting to use a deprecated \"pplx\" model will result in an API error, and users should verify they are referencing the latest supported model names as documented in the current API model cards.\n",
      "3. Third-party integrations may not have updated their model lists in line with Perplexity’s changes, so users may need to update dependencies or select supported models from the latest API documentation.\n",
      "\n",
      "next steps\n",
      "1. Update your applications and API calls to use the supported \"Sonar\" models instead of any \"pplx-\" series models.\n",
      "2. Check and synchronize any third-party integrations or custom platforms to ensure they reference the current official model names as per the latest Perplexity API documentation.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Replace 'YOUR_PERPLEXITY_API_KEY' with your actual API key\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",\n",
    "    base_url=\"https://api.perplexity.ai\"  # Important: set Perplexity base URL!\n",
    ")\n",
    "\n",
    "# The prompt for summarization\n",
    "prompt = \"\"\"Summarize the following text in a concise paragraph:\\n\\n\n",
    "Model Deprecation or Removal: The pplx-7b-online model was publicly available, but as of March 2025, Perplexity is deprecating or has already phased out the \"pplx-\" series models (including pplx-7b-online and pplx-70b-online). Users are now instructed to use the newer \"Sonar\" models (such as sonar-7b-online, sonar-7b, or sonar-70b) instead. If you attempt to use a deprecated \"pplx\" model, the API will return this error.\n",
    "\n",
    "Incorrect Model Name or Endpoint: Verify that you are specifying the correct and currently supported model name as documented in the latest Perplexity API model cards documentation. The current official models often have names like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
    "\n",
    "API Platform Sync Issues: If you are using a third-party integration (like Make, Langchain, or a custom platform), it may not have updated its model list in sync with Perplexity’s changes. In this case, updating your dependencies or choosing from the supported model list in the API docs should fix the issue..\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"sonar-pro\",  # Or try 'gpt-4o', 'sonar-medium-online', etc.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Your are a helpful assistant that provides  summary in pointsand next steps.\n",
    "        output format \n",
    "        summary\n",
    "        1.point one\n",
    "        2.point two\n",
    "        next steps\n",
    "        1.point one\n",
    "        2.point two\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43d64894-534a-425b-9de1-d234432eab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Setup Perplexity client\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  # 🔑 Add your actual API key\n",
    "    base_url=\"https://api.perplexity.ai\"\n",
    ")\n",
    "# [\"gpt-4.1\", \"gemini-2.5-pro\", \"\", \"o3\"]\n",
    "def summarize_with_perplexity(text: str, model: str = \"sonar-pro\") -> str:\n",
    "    \"\"\"\n",
    "    Summarizes the given text using Perplexity's API in bullet-point format.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to summarize.\n",
    "        model (str): The model to use (default: \"sonar-pro\").\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted summary and next steps.\n",
    "    \"\"\"\n",
    "    print(f\"hi {model}eshawr\")\n",
    "    print(f\"....{text}....\")\n",
    "    # prompt = f\"\"\"Summarize the following text in a concise paragraph:\\n\\n{text}\"\"\"\n",
    "    prompt = f\"\"\"{text}\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant that provides summary in points and next steps.\\n\"\n",
    "                    \"Output format:\\n\"\n",
    "                    \"summary\\n\"\n",
    "                    \"1. point one\\n\"\n",
    "                    \"2. point two\\n\"\n",
    "                    \"next steps\\n\"\n",
    "                    \"1. point one\\n\"\n",
    "                    \"2. point two\"\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d50ae57-f847-461c-97fd-60c85f49a034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi sonar-proeshawr\n",
      "....\n",
      "Model Deprecation or Removal: The pplx-7b-online model was publicly available, \n",
      "but as of March 2025, Perplexity is deprecating or has already phased out the \n",
      "\"pplx-\" series models (including pplx-7b-online and pplx-70b-online). \n",
      "Users are now instructed to use the newer \"Sonar\" models \n",
      "(such as sonar-7b-online, sonar-7b, or sonar-70b) instead.\n",
      "If you attempt to use a deprecated \"pplx\" model, the API will return this error.\n",
      "\n",
      "Incorrect Model Name or Endpoint: Verify that you are specifying the correct \n",
      "and currently supported model name as documented in the latest Perplexity \n",
      "API model cards documentation. The current official models often have names \n",
      "like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
      "\n",
      "API Platform Sync Issues: If you are using a third-party integration \n",
      "(like Make, Langchain, or a custom platform), it may not have updated its \n",
      "model list in sync with Perplexity’s changes. In this case, updating your \n",
      "dependencies or choosing from the supported model list in the API docs should fix the issue.\n",
      "....\n",
      "summary\n",
      "1. Perplexity has deprecated the \"pplx-\" series models (including pplx-7b-online and pplx-70b-online) as of early 2025; these models are no longer accessible via the API[1].\n",
      "2. Users are now required to use the newer \"Sonar\" models (such as sonar-7b-online, sonar-7b, sonar-70b, or Sonar Pro), which are the officially supported models for Perplexity’s API[1][2].\n",
      "3. Attempting to use a deprecated \"pplx\" model will result in an API error indicating the model is unavailable[1].\n",
      "4. If you are using a third-party integration or platform, it may not have updated its model list in sync with Perplexity’s changes, potentially causing compatibility issues[1].\n",
      "\n",
      "next steps\n",
      "1. Update your application or integration to use one of the supported Sonar models (e.g., sonar-7b-online, sonar-70b, sonar-pro) as documented in the latest Perplexity API model cards[1][2].\n",
      "2. If using a third-party platform (such as Make, Langchain, etc.), ensure your dependencies and model selections are updated to reflect the current supported models; consult the Perplexity API documentation for the latest model names and endpoints[1].\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Model Deprecation or Removal: The pplx-7b-online model was publicly available, \n",
    "but as of March 2025, Perplexity is deprecating or has already phased out the \n",
    "\"pplx-\" series models (including pplx-7b-online and pplx-70b-online). \n",
    "Users are now instructed to use the newer \"Sonar\" models \n",
    "(such as sonar-7b-online, sonar-7b, or sonar-70b) instead.\n",
    "If you attempt to use a deprecated \"pplx\" model, the API will return this error.\n",
    "\n",
    "Incorrect Model Name or Endpoint: Verify that you are specifying the correct \n",
    "and currently supported model name as documented in the latest Perplexity \n",
    "API model cards documentation. The current official models often have names \n",
    "like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
    "\n",
    "API Platform Sync Issues: If you are using a third-party integration \n",
    "(like Make, Langchain, or a custom platform), it may not have updated its \n",
    "model list in sync with Perplexity’s changes. In this case, updating your \n",
    "dependencies or choosing from the supported model list in the API docs should fix the issue.\n",
    "\"\"\"\n",
    "\n",
    "# Run the function\n",
    "summary_output = summarize_with_perplexity(sample_text)\n",
    "print(summary_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02c0d0f0-fd71-4bd2-a9d6-6cb35d78e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before calling\n",
      "📤 [DEBUG] Starting summarization with Perplexity\n",
      "🧠 Model: sonar-pro2\n",
      "📝 Input Text:\n",
      "\n",
      "Model Deprecation or Removal: The pplx-7b-online model was publicly available, \n",
      "but as of March 2025, Perplexity is deprecating or has already phased out the \n",
      "'pplx-' series models (including pplx-7b-online and pplx-70b-online). \n",
      "Users are now instructed to use the newer 'Sonar' models \n",
      "(such as sonar-7b-online, sonar-7b, or sonar-70b) instead.\n",
      "If you attempt to use a deprecated 'pplx' model, the API will return this error.\n",
      "\n",
      "Incorrect Model Name or Endpoint: Verify that you are specifying the correct \n",
      "and currently supported model name as documented in the latest Perplexity \n",
      "API model cards documentation. The current official models often have names \n",
      "like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
      "\n",
      "API Platform Sync Issues: If you are using a third-party integration \n",
      "(like Make, Langchain, or a custom platform), it may not have updated its \n",
      "model list in sync with Perplexity’s changes. In this case, updating your \n",
      "dependencies or choosing from the supported model list in the API docs should fix the issue.\n",
      "\n",
      "\n",
      "❌ [ERROR] Failed to get a response: Error code: 400 - {'error': {'message': \"Invalid model 'sonar-pro2'. Permitted models can be found in the documentation at https://docs.perplexity.ai/guides/model-cards.\", 'type': 'invalid_model', 'code': 400}}\n",
      "\n",
      "📄 Final Summary Output:\n",
      "\n",
      "Error: Failed to summarize the input.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 1️⃣ Setup Perplexity Client\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  # 🔐 Your API key\n",
    "    base_url=\"https://api.perplexity.ai\"  # 🧠 Must be Perplexity endpoint\n",
    ")\n",
    "\n",
    "# 2️⃣ Summarization Function\n",
    "def summarize_with_perplexity(text: str, model: str = \"sonar-pro\") -> str:\n",
    "    \"\"\"\n",
    "    Summarizes input text using Perplexity API and a given model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The raw text to summarize.\n",
    "        model (str): The LLM model to use (default: \"sonar-pro\").\n",
    "    \n",
    "    Returns:\n",
    "        str: A structured summary with next steps.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"📤 [DEBUG] Starting summarization with Perplexity\")\n",
    "    print(f\"🧠 Model: {model}\")\n",
    "    print(f\"📝 Input Text:\\n{text}\\n\")\n",
    "\n",
    "    # 3️⃣ Create the system prompt to instruct the model\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that provides summary in points and next steps.\\n\"\n",
    "        \"Output format:\\n\"\n",
    "        \"summary\\n\"\n",
    "        \"1. point one\\n\"\n",
    "        \"2. point two\\n\"\n",
    "        \"next steps\\n\"\n",
    "        \"1. point one\\n\"\n",
    "        \"2. point two\"\n",
    "    )\n",
    "\n",
    "    # 4️⃣ User message is just the raw text\n",
    "    user_prompt = text\n",
    "\n",
    "    # 5️⃣ Call the API\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        # 6️⃣ Extract and return response\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(\"\\n✅ [DEBUG] Response received successfully\")\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ [ERROR] Failed to get a response:\", e)\n",
    "        return \"Error: Failed to summarize the input.\"\n",
    "\n",
    "# 7️⃣ Sample Text to Test\n",
    "sample_text = \"\"\"\n",
    "Model Deprecation or Removal: The pplx-7b-online model was publicly available, \n",
    "but as of March 2025, Perplexity is deprecating or has already phased out the \n",
    "'pplx-' series models (including pplx-7b-online and pplx-70b-online). \n",
    "Users are now instructed to use the newer 'Sonar' models \n",
    "(such as sonar-7b-online, sonar-7b, or sonar-70b) instead.\n",
    "If you attempt to use a deprecated 'pplx' model, the API will return this error.\n",
    "\n",
    "Incorrect Model Name or Endpoint: Verify that you are specifying the correct \n",
    "and currently supported model name as documented in the latest Perplexity \n",
    "API model cards documentation. The current official models often have names \n",
    "like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
    "\n",
    "API Platform Sync Issues: If you are using a third-party integration \n",
    "(like Make, Langchain, or a custom platform), it may not have updated its \n",
    "model list in sync with Perplexity’s changes. In this case, updating your \n",
    "dependencies or choosing from the supported model list in the API docs should fix the issue.\n",
    "\"\"\"\n",
    "\n",
    "# 8️⃣ Run the Function and Print Result\n",
    "print(f\"before calling\")\n",
    "summary_output = summarize_with_perplexity(sample_text,'sonar-pro2')\n",
    "print(\"\\n📄 Final Summary Output:\\n\")\n",
    "print(summary_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf15dfab-557d-41e5-a38d-99869a2af478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before calling\n",
      "📤 [DEBUG] Starting summarization with Perplexity\n",
      "🧠 Model: sonar-pro2\n",
      "📝 Input Text:\n",
      "\n",
      "Model Deprecation or Removal: The pplx-7b-online model was publicly available, \n",
      "but as of March 2025, Perplexity is deprecating or has already phased out the \n",
      "'pplx-' series models (including pplx-7b-online and pplx-70b-online). \n",
      "Users are now instructed to use the newer 'Sonar' models \n",
      "(such as sonar-7b-online, sonar-7b, or sonar-70b) instead.\n",
      "If you attempt to use a deprecated 'pplx' model, the API will return this error.\n",
      "\n",
      "Incorrect Model Name or Endpoint: Verify that you are specifying the correct \n",
      "and currently supported model name as documented in the latest Perplexity \n",
      "API model cards documentation. The current official models often have names \n",
      "like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
      "\n",
      "API Platform Sync Issues: If you are using a third-party integration \n",
      "(like Make, Langchain, or a custom platform), it may not have updated its \n",
      "model list in sync with Perplexity’s changes. In this case, updating your \n",
      "dependencies or choosing from the supported model list in the API docs should fix the issue.\n",
      "\n",
      "\n",
      "❌ [ERROR] Failed to get a response: Error code: 400 - {'error': {'message': \"Invalid model 'sonar-pro2'. Permitted models can be found in the documentation at https://docs.perplexity.ai/guides/model-cards.\", 'type': 'invalid_model', 'code': 400}}\n",
      "\n",
      "📄 Final Summary Output:\n",
      "\n",
      "Error: Failed to summarize the input.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 1️⃣ Setup Perplexity Client\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  # 🔐 Your API key\n",
    "    base_url=\"https://api.perplexity.ai\"  # 🧠 Must be Perplexity endpoint\n",
    ")\n",
    "\n",
    "# 2️⃣ Summarization Function\n",
    "def summarize_with_perplexity(text: str, model: str = \"sonar-pro\") -> str:\n",
    "    \"\"\"\n",
    "    Summarizes input text using Perplexity API and a given model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The raw text to summarize.\n",
    "        model (str): The LLM model to use (default: \"sonar-pro\").\n",
    "    \n",
    "    Returns:\n",
    "        str: A structured summary with next steps.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"📤 [DEBUG] Starting summarization with Perplexity\")\n",
    "    print(f\"🧠 Model: {model}\")\n",
    "    print(f\"📝 Input Text:\\n{text}\\n\")\n",
    "\n",
    "    # 3️⃣ Create the system prompt to instruct the model\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that provides summary in points and next steps.\\n\"\n",
    "        \"Output format:\\n\"\n",
    "        \"summary\\n\"\n",
    "        \"1. point one\\n\"\n",
    "        \"2. point two\\n\"\n",
    "        \"next steps\\n\"\n",
    "        \"1. point one\\n\"\n",
    "        \"2. point two\"\n",
    "    )\n",
    "\n",
    "    # 4️⃣ User message is just the raw text\n",
    "    user_prompt = text\n",
    "\n",
    "    # 5️⃣ Call the API\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        # 6️⃣ Extract and return response\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(\"\\n✅ [DEBUG] Response received successfully\")\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ [ERROR] Failed to get a response:\", e)\n",
    "        return \"Error: Failed to summarize the input.\"\n",
    "\n",
    "# 7️⃣ Sample Text to Test\n",
    "sample_text = \"\"\"\n",
    "Model Deprecation or Removal: The pplx-7b-online model was publicly available, \n",
    "but as of March 2025, Perplexity is deprecating or has already phased out the \n",
    "'pplx-' series models (including pplx-7b-online and pplx-70b-online). \n",
    "Users are now instructed to use the newer 'Sonar' models \n",
    "(such as sonar-7b-online, sonar-7b, or sonar-70b) instead.\n",
    "If you attempt to use a deprecated 'pplx' model, the API will return this error.\n",
    "\n",
    "Incorrect Model Name or Endpoint: Verify that you are specifying the correct \n",
    "and currently supported model name as documented in the latest Perplexity \n",
    "API model cards documentation. The current official models often have names \n",
    "like sonar, sonar-pro, or llama-3.1-sonar-huge-128k-online, not pplx-7b-online.\n",
    "\n",
    "API Platform Sync Issues: If you are using a third-party integration \n",
    "(like Make, Langchain, or a custom platform), it may not have updated its \n",
    "model list in sync with Perplexity’s changes. In this case, updating your \n",
    "dependencies or choosing from the supported model list in the API docs should fix the issue.\n",
    "\"\"\"\n",
    "\n",
    "# 8️⃣ Run the Function and Print Result\n",
    "print(f\"before calling\")\n",
    "summary_output = summarize_with_perplexity(sample_text,'sonar-pro2')\n",
    "print(\"\\n📄 Final Summary Output:\\n\")\n",
    "print(summary_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99e2e633-5177-49a4-9f11-aed65be2df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before calling\n",
      "📤 [DEBUG] Starting summarization with Perplexity\n",
      "🧠 Model: sonar-pro\n",
      "📝 Input Text:\n",
      "\n",
      "Mysore's history is rich, stemming from a legend about the demon Mahishasura and evolving into a prominent kingdom under the Wodeyar dynasty. The city, initially called Mahishapura, is named after the demon and its association with Goddess Chamundeshwari, who slayed him. The Wodeyars, who ruled the Kingdom of Mysore, played a significant role in shaping the city's cultural and architectural landscape, including the iconic Mysore Palace. \n",
      "Here's a more detailed look:\n",
      "Ancient Roots:\n",
      "Mysore's history is intertwined with the legend of Mahishasura, a buffalo-headed demon believed to have been slain by Goddess Chamundeshwari atop Chamundi Hill. \n",
      "Early Kingdom:\n",
      "The Kingdom of Mysore, founded by Yaduraya and Krishnaraya, initially centered around the city and was later ruled by the Wodeyar dynasty. \n",
      "Wodeyar Dynasty:\n",
      "The Wodeyars, also known as Wadiyars, were instrumental in developing Mysore into a cultural and architectural hub. \n",
      "Hyder Ali and Tipu Sultan:\n",
      "Hyder Ali, initially a commander, rose to power and briefly held the title of \"Nawab of Mysore\" before Tipu Sultan, his son, took over. \n",
      "\n",
      "\n",
      "\n",
      "✅ [DEBUG] Response received successfully\n",
      "\n",
      "📄 Final Summary Output:\n",
      "\n",
      "summary\n",
      "1. Mysore's history is rooted in the legend of Mahishasura, a demon slain by Goddess Chamundeshwari, giving the city its name and religious significance[1].\n",
      "2. The Kingdom of Mysore was founded by Yaduraya and Krishnaraya, with the Wodeyar (Wadiyar) dynasty shaping its cultural and architectural legacy, notably the Mysore Palace[1][2].\n",
      "3. The Wodeyars initially ruled as vassals of the Vijayanagara Empire, gaining independence after its decline in 1565 and expanding their territory significantly under rulers like Narasaraja Wodeyar and Chikka Devaraja Wodeyar[2].\n",
      "4. In the late 18th century, Hyder Ali, originally a commander, seized power, and his son Tipu Sultan continued to rule, modernizing the military and expanding the kingdom until Tipu's defeat by the British in 1799[3][4].\n",
      "\n",
      "next steps\n",
      "1. Explore primary sources such as inscriptions, palace records, and local literature for deeper insights into Mysore's early history and the Wodeyar dynasty's origins[1].\n",
      "2. Study the architectural and cultural developments under the Wodeyars, focusing on landmarks like Mysore Palace and their patronage of the arts.\n",
      "3. Examine the political and military strategies of Hyder Ali and Tipu Sultan, including their conflicts with regional and colonial powers\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "ient = OpenAI(\n",
    "    api_key=\"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\",\n",
    "    base_url=\"clhttps://api.perplexity.ai\"\n",
    ")\n",
    "\n",
    "def summarize_with_perplexity(text: str, model: str = \"sonar-pro\") -> str:\n",
    "    print(\"📤 [DEBUG] Starting summarization with Perplexity\")\n",
    "    print(f\"🧠 Model: {model}\")\n",
    "    print(f\"📝 Input Text:\\n{text}\\n\")\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that provides summary in points and next steps.\\n\"\n",
    "        \"Output format:\\n\"\n",
    "        \"summary\\n\"\n",
    "        \"1. point one\\n\"\n",
    "        \"2. point two\\n\"\n",
    "        \"next steps\\n\"\n",
    "        \"1. point one\\n\"\n",
    "        \"2. point two\"\n",
    "    )\n",
    "\n",
    "    user_prompt = text\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(\"\\n✅ [DEBUG] Response received successfully\")\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ [ERROR] Failed to get a response:\", e)\n",
    "        return \"Error: Failed to summarize the input.\"\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Mysore's history is rich, stemming from a legend about the demon Mahishasura and evolving into a prominent kingdom under the Wodeyar dynasty. The city, initially called Mahishapura, is named after the demon and its association with Goddess Chamundeshwari, who slayed him. The Wodeyars, who ruled the Kingdom of Mysore, played a significant role in shaping the city's cultural and architectural landscape, including the iconic Mysore Palace. \n",
    "Here's a more detailed look:\n",
    "Ancient Roots:\n",
    "Mysore's history is intertwined with the legend of Mahishasura, a buffalo-headed demon believed to have been slain by Goddess Chamundeshwari atop Chamundi Hill. \n",
    "Early Kingdom:\n",
    "The Kingdom of Mysore, founded by Yaduraya and Krishnaraya, initially centered around the city and was later ruled by the Wodeyar dynasty. \n",
    "Wodeyar Dynasty:\n",
    "The Wodeyars, also known as Wadiyars, were instrumental in developing Mysore into a cultural and architectural hub. \n",
    "Hyder Ali and Tipu Sultan:\n",
    "Hyder Ali, initially a commander, rose to power and briefly held the title of \"Nawab of Mysore\" before Tipu Sultan, his son, took over. \n",
    "\"\"\"\n",
    "\n",
    "print(\"before calling\")\n",
    "summary_output = summarize_with_perplexity(sample_text, 'sonar-pro')\n",
    "print(\"\\n📄 Final Summary Output:\\n\")\n",
    "print(summary_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36c5da-cf35-45cd-b322-acabcdee816c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
